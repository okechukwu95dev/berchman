name: FlashScore Scraper

on:
  schedule:
    - cron: '0 0 * * *'  # Run daily at midnight UTC
  workflow_dispatch:     # Allow manual triggering
    inputs:
      league_url:
        description: 'League URL to scrape (e.g. https://www.flashscore.com/football/scotland/premiership/standings)'
        required: true
        type: string

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'

      - name: Install dependencies
        run: npm install puppeteer

      - name: Create package.json with type module
        run: |
          echo '{
            "name": "flashscore-scraper",
            "version": "1.0.0",
            "type": "module",
            "dependencies": {
              "puppeteer": "^21.5.0"
            }
          }' > package.json

      - name: Create scraper script
        run: |
          cat > scraper.js << 'EOL'
          import puppeteer from 'puppeteer';
          import fs from 'fs/promises';
          import path from 'path';
          import { performance } from 'perf_hooks';

          // === CONFIG ===
          const DEFAULT_TIMEOUT = 3000;
          const CUP_TIMEOUT = 2000;
          const OUTPUT_DIR = './data';

          // === HELPERS ===
          const isCup = url => /cup|copa|trophy|shield|knockout/i.test(url);
          const timestamp = () => new Date().toISOString().split('T')[0].replace(/-/g, '');

          // Extract league name from URL
          function getLeagueNameFromUrl(url) {
            // Extract parts from URL
            const parts = url.split('/');
            // Find the league part - typically after "football" and country
            for (let i = 0; i < parts.length; i++) {
              if (parts[i] === 'football' && i + 2 < parts.length) {
                return parts[i+2]; // Country is i+1, league is i+2
              }
            }
            return 'unknown-league';
          }

          // Fetch teams for a league URL
          async function fetchTeams(browser, leagueUrl) {
            console.log(`‚û°Ô∏è Fetching teams for: ${leagueUrl}`);

            // Make sure we're looking at standings page
            const standingsUrl = leagueUrl.includes('/standings')
              ? leagueUrl
              : `${leagueUrl.replace(/\/+$/, '')}/standings`;

            const timeout = isCup(standingsUrl) ? CUP_TIMEOUT : DEFAULT_TIMEOUT;
            const page = await browser.newPage();

            try {
              // Set user agent to avoid detection
              await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36');

              // Go to standings page
              console.log(`Navigating to: ${standingsUrl}`);
              await page.goto(standingsUrl, {
                waitUntil: 'domcontentloaded',
                timeout: 30000
              });

              // Wait for team links to appear
              await page.waitForSelector('a[href*="/team/"]', { timeout });

              // Extract teams
              const teams = await page.evaluate(() => {
                const map = new Map();
                document.querySelectorAll('a[href*="/team/"]').forEach(a => {
                  const match = a.href.match(/\/team\/[^\/]+\/([^\/?#]+)/);
                  const name = a.innerText.trim();

                  if (match && name && !map.has(match[1])) {
                    map.set(match[1], {
                      name,
                      id: match[1],
                      url: a.href
                    });
                  }
                });

                return [...map.values()];
              });

              console.log(`‚úÖ Found ${teams.length} teams`);
              await page.close();
              return teams;
            } catch (error) {
              console.warn(`‚ö†Ô∏è Error fetching teams: ${error.message}`);
              await page.close();

              // Return placeholder for cup competitions
              if (isCup(leagueUrl)) {
                return [{ name: 'isCup', id: 'isCup', url: null }];
              }

              return [];
            }
          }

          // Main function
          async function main() {
            const leagueUrl = process.argv[2];
            if (!leagueUrl) {
              console.error('‚ùå Missing league URL!');
              process.exit(1);
            }

            console.log(`üöÄ Starting scrape for: ${leagueUrl}`);
            const start = performance.now();

            // Create output directory
            await fs.mkdir(OUTPUT_DIR, { recursive: true });

            // Generate output filename
            const leagueName = getLeagueNameFromUrl(leagueUrl);
            const ts = timestamp();
            const outputFile = path.join(OUTPUT_DIR, `${leagueName}-teams-${ts}.json`);

            // Launch browser
            const browser = await puppeteer.launch({
              headless: true,
              args: [
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage'
              ]
            });

            try {
              // Get teams
              const teams = await fetchTeams(browser, leagueUrl);

              if (teams.length === 0) {
                console.error('‚ùå No teams found!');
                process.exit(1);
              }

              // Save data
              const data = {
                league: {
                  name: leagueName,
                  url: leagueUrl,
                  scrapedAt: new Date().toISOString()
                },
                teams: teams,
                stats: {
                  teamCount: teams.length,
                  duration: ((performance.now() - start) / 1000).toFixed(1) + 's'
                }
              };

              await fs.writeFile(outputFile, JSON.stringify(data, null, 2));
              console.log(`üíæ Data saved to: ${outputFile}`);

              const duration = ((performance.now() - start) / 1000).toFixed(1);
              console.log(`‚úÖ Completed in ${duration}s`);
            } catch (error) {
              console.error(`‚ùå Error: ${error.message}`);
              process.exit(1);
            } finally {
              await browser.close();
            }
          }

          // Run main function
          main().catch(error => {
            console.error(`‚ùå Fatal error: ${error.message}`);
            process.exit(1);
          });
          EOL

      - name: Create data directory
        run: mkdir -p data

      - name: Run scraper
        run: node scraper.js "${{ github.event.inputs.league_url }}"

      - name: Commit and push changes
        run: |
          git config --global user.name 'GitHub Action Bot'
          git config --global user.email 'action@github.com'
          git add data/
          git diff --quiet && git diff --staged --quiet || (git commit -m "Update FlashScore league data [$(date +'%Y-%m-%d')]" && git push)
